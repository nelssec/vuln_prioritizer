#!/usr/bin/env python3
"""
Vulnerability Prioritization System v3
- Correct EPSS API implementation
- Database caching for API calls
- Support for multiple data sources (Tenable, Qualys, Nessus, etc.)
- No emojis or icons in output
"""

import json
import requests
import csv
from datetime import datetime, timedelta
from enum import Enum
from typing import List, Dict, Optional
from dataclasses import dataclass
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import xml.etree.ElementTree as ET
from io import StringIO
from epss_cache_db import EPSSCacheDB


class DataSource(Enum):
    """Supported vulnerability data sources"""
    NESSUS = "nessus"
    TENABLE = "tenable"
    QUALYS = "qualys"
    RAPID7 = "rapid7"
    GENERIC_CSV = "generic_csv"
    GENERIC_JSON = "generic_json"


class AssetCriticality(Enum):
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    MINIMAL = 1


class ExposureLevel(Enum):
    INTERNET = 3
    EXTRANET = 2
    INTERNAL = 1


class DataSensitivity(Enum):
    HIGHLY_SENSITIVE = 3
    SENSITIVE = 2
    PUBLIC = 1


class RiskLevel(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    MINIMAL = "MINIMAL"


@dataclass
class Vulnerability:
    cve_id: str
    cvss_score: float
    cvss_version: str = "3.1"
    affected_assets: List[str] = None
    asset_criticality: AssetCriticality = AssetCriticality.MEDIUM
    exposure_level: ExposureLevel = ExposureLevel.INTERNAL
    data_sensitivity: DataSensitivity = DataSensitivity.PUBLIC
    description: str = ""
    epss_score: Optional[float] = None
    epss_percentile: Optional[float] = None
    in_cisa_kev: bool = False
    exploit_available: bool = False
    actively_exploited: bool = False
    ransomware_campaign: bool = False
    published_date: Optional[datetime] = None
    last_modified_date: Optional[datetime] = None
    risk_score: float = 0.0
    risk_level: RiskLevel = RiskLevel.MEDIUM
    priority_rank: int = 0
    plugin_id: str = ""
    port: str = ""
    protocol: str = ""
    source: DataSource = DataSource.NESSUS

    def __post_init__(self):
        if self.affected_assets is None:
            self.affected_assets = []


class VulnerabilityPrioritizer:
    def __init__(self, config_file: str = "prioritizer_config.json", 
                 use_cache: bool = True, cache_db_path: str = "epss_cache.db"):
        """
        Initialize the Vulnerability Prioritizer
        
        Args:
            config_file: Path to configuration JSON file
            use_cache: Enable database caching for API calls
            cache_db_path: Path to SQLite cache database
        """
        self.config = self.load_config(config_file)
        self.use_cache = use_cache
        
        # Initialize database cache
        if use_cache:
            self.db = EPSSCacheDB(cache_db_path)
            print(f"[INFO] Using database cache at {cache_db_path}")
        else:
            self.db = None
            print("[INFO] Database caching disabled")
        
        # In-memory caches for this session
        self.epss_cache = {}
        self.cisa_kev_cache = set()
        
    def load_config(self, config_file: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"[INFO] Config file {config_file} not found, using defaults")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """Return default configuration"""
        return {
            "prioritization_weights": {
                "cvss_weight": 0.20,
                "epss_weight": 0.25,
                "asset_context_weight": 0.20,
                "threat_intel_weight": 0.25,
                "temporal_weight": 0.10
            },
            "asset_classification": {
                "critical_indicators": ["prod", "payment", "customer", "database", "auth"],
                "internet_facing_multiplier": 2.0,
                "data_sensitivity_multiplier": 1.5
            },
            "patch_management": {
                "critical_sla_hours": 24,
                "high_sla_hours": 168,
                "medium_sla_hours": 720
            },
            "api_settings": {
                "epss_api": "https://api.first.org/data/v1/epss",
                "cisa_kev_api": "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json",
                "timeout": 10,
                "max_retries": 3,
                "rate_limit_delay": 1.0
            },
            "cache_settings": {
                "epss_cache_days": 7,
                "kev_cache_days": 1
            }
        }
    
    def fetch_epss_scores(self, cve_ids: List[str]) -> Dict[str, Dict]:
        """
        Fetch EPSS scores from FIRST.org API with database caching
        
        API Documentation: https://www.first.org/epss/api
        Base URL: https://api.first.org/data/v1/epss
        """
        print(f"[INFO] Fetching EPSS scores for {len(cve_ids)} CVEs...")
        epss_data = {}
        uncached_cves = []
        
        # Check database cache first
        if self.db:
            cache_hits = 0
            for cve in cve_ids:
                cached = self.db.get_epss_score(cve, 
                    max_age_days=self.config['cache_settings']['epss_cache_days'])
                if cached:
                    epss_data[cve] = cached
                    self.epss_cache[cve] = cached
                    cache_hits += 1
                else:
                    uncached_cves.append(cve)
            
            if cache_hits > 0:
                print(f"[CACHE] Found {cache_hits} CVEs in cache")
        else:
            uncached_cves = cve_ids
        
        if not uncached_cves:
            print("[INFO] All EPSS scores found in cache")
            return epss_data
        
        # Fetch from API in batches
        print(f"[API] Fetching {len(uncached_cves)} CVEs from EPSS API...")
        batch_size = 30  # API supports batch requests
        success_count = 0
        
        for i in range(0, len(uncached_cves), batch_size):
            batch = uncached_cves[i:i + batch_size]
            cve_param = ','.join(batch)
            
            try:
                start_time = time.time()
                url = f"{self.config['api_settings']['epss_api']}?cve={cve_param}"
                
                headers = {
                    'User-Agent': 'VulnPrioritizer/3.0',
                    'Accept': 'application/json'
                }
                
                response = requests.get(
                    url, 
                    headers=headers,
                    timeout=self.config['api_settings']['timeout']
                )
                
                response_time = time.time() - start_time
                
                # Log API call
                if self.db:
                    self.db.log_api_call(
                        "EPSS",
                        url,
                        {"cve_count": len(batch)},
                        response.status_code,
                        response_time,
                        False
                    )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Parse response according to FIRST.org API format
                    for item in data.get('data', []):
                        cve = item['cve']
                        epss_score = float(item['epss'])
                        percentile = float(item['percentile'])
                        model_version = data.get('model-version', '')
                        score_date = data.get('score-date', '')
                        
                        epss_info = {
                            'epss': epss_score,
                            'percentile': percentile,
                            'model_version': model_version,
                            'score_date': score_date
                        }
                        
                        epss_data[cve] = epss_info
                        self.epss_cache[cve] = epss_info
                        
                        # Cache in database
                        if self.db:
                            self.db.cache_epss_score(
                                cve, epss_score, percentile, 
                                model_version, score_date
                            )
                        
                        success_count += 1
                    
                    print(f"[API] Batch {i//batch_size + 1}: Retrieved {len(batch)} CVEs")
                else:
                    print(f"[ERROR] EPSS API returned status {response.status_code} for batch {i//batch_size + 1}")
                    # Set default values for failed CVEs
                    for cve in batch:
                        if cve not in epss_data:
                            epss_data[cve] = {'epss': 0.0, 'percentile': 0.0, 
                                            'model_version': '', 'score_date': ''}
                            
            except Exception as e:
                print(f"[ERROR] Error fetching EPSS data for batch: {e}")
                # Set default values for failed CVEs
                for cve in batch:
                    if cve not in epss_data:
                        epss_data[cve] = {'epss': 0.0, 'percentile': 0.0, 
                                        'model_version': '', 'score_date': ''}
            
            # Rate limiting
            time.sleep(self.config['api_settings']['rate_limit_delay'])
        
        print(f"[INFO] Successfully fetched EPSS for {success_count}/{len(uncached_cves)} CVEs from API")
        
        # Ensure all requested CVEs have data
        for cve in cve_ids:
            if cve not in epss_data:
                epss_data[cve] = {'epss': 0.0, 'percentile': 0.0, 
                                'model_version': '', 'score_date': ''}
        
        return epss_data
    
    def fetch_cisa_kev(self) -> set:
        """Fetch CISA Known Exploited Vulnerabilities catalog with database caching"""
        # Check memory cache first
        if self.cisa_kev_cache:
            return self.cisa_kev_cache
        
        # Check database cache
        if self.db:
            cached_kevs = self.db.get_all_cisa_kev_cves(
                max_age_days=self.config['cache_settings']['kev_cache_days']
            )
            if cached_kevs:
                print(f"[CACHE] Loaded {len(cached_kevs)} CVEs from CISA KEV cache")
                self.cisa_kev_cache = cached_kevs
                return self.cisa_kev_cache
        
        print("[API] Fetching CISA KEV catalog...")
        try:
            start_time = time.time()
            headers = {
                'User-Agent': 'VulnPrioritizer/3.0',
                'Accept': 'application/json'
            }
            
            response = requests.get(
                self.config['api_settings']['cisa_kev_api'],
                headers=headers,
                timeout=self.config['api_settings']['timeout']
            )
            
            response_time = time.time() - start_time
            
            # Log API call
            if self.db:
                self.db.log_api_call(
                    "CISA_KEV",
                    self.config['api_settings']['cisa_kev_api'],
                    {},
                    response.status_code,
                    response_time,
                    False
                )
            
            if response.status_code == 200:
                data = response.json()
                vulnerabilities = data.get('vulnerabilities', [])
                
                for vuln in vulnerabilities:
                    cve_id = vuln.get('cveID', '')
                    if cve_id:
                        self.cisa_kev_cache.add(cve_id)
                        
                        # Cache in database
                        if self.db:
                            self.db.cache_cisa_kev(
                                cve_id,
                                vuln.get('vendorProject', ''),
                                vuln.get('product', ''),
                                vuln.get('vulnerabilityName', ''),
                                vuln.get('dateAdded', ''),
                                vuln.get('shortDescription', ''),
                                vuln.get('requiredAction', ''),
                                vuln.get('dueDate', '')
                            )
                
                print(f"[INFO] Loaded {len(self.cisa_kev_cache)} CVEs from CISA KEV")
            else:
                print(f"[ERROR] Failed to fetch CISA KEV: {response.status_code}")
                
        except Exception as e:
            print(f"[ERROR] Error fetching CISA KEV: {e}")
            
        return self.cisa_kev_cache
    
    def import_nessus(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Nessus .nessus XML file"""
        print(f"\n[INFO] Importing Nessus scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.NESSUS)
    
    def import_tenable(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Tenable export file"""
        print(f"\n[INFO] Importing Tenable scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.TENABLE)
    
    def import_qualys(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Qualys export file"""
        print(f"\n[INFO] Importing Qualys scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.QUALYS)
    
    def _import_vulnerability_scan(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Generic vulnerability scan importer"""
        if source == DataSource.NESSUS or source == DataSource.TENABLE:
            return self._parse_nessus_xml(filepath, source)
        elif source == DataSource.QUALYS:
            # Detect if it's CSV or XML based on file extension
            if filepath.lower().endswith('.csv'):
                return self._parse_qualys_csv(filepath, source)
            else:
                return self._parse_qualys_xml(filepath, source)
        else:
            print(f"[ERROR] Unsupported source: {source}")
            return []
    
    def _parse_nessus_xml(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Parse Nessus/Tenable XML format"""
        vulnerabilities = []
        cve_map = {}
        
        try:
            tree = ET.parse(filepath)
            root = tree.getroot()
            
            for host in root.findall('.//ReportHost'):
                hostname = host.get('name')
                asset_crit = self.classify_asset(hostname)
                
                for item in host.findall('.//ReportItem'):
                    severity = int(item.get('severity', 0))
                    if severity == 0:
                        continue
                    
                    cves = [cve.text for cve in item.findall('.//cve') if cve.text]
                    
                    if not cves:
                        continue
                    
                    for cve_id in cves:
                        cvss_elem = item.find('.//cvss3_base_score')
                        if cvss_elem is None:
                            cvss_elem = item.find('.//cvss_base_score')
                            cvss_version = "2.0"
                        else:
                            cvss_version = "3.1"
                        
                        cvss_score = float(cvss_elem.text) if cvss_elem is not None else 0.0
                        
                        if cvss_score == 0.0:
                            continue
                        
                        if cve_id not in cve_map:
                            cve_map[cve_id] = {
                                'cvss_score': cvss_score,
                                'cvss_version': cvss_version,
                                'assets': [],
                                'description': item.find('.//description').text if item.find('.//description') is not None else "",
                                'plugin_id': item.get('pluginID', ''),
                                'port': item.get('port', ''),
                                'protocol': item.get('protocol', ''),
                                'max_criticality': asset_crit
                            }
                        
                        cve_map[cve_id]['assets'].append(hostname)
                        
                        if asset_crit.value > cve_map[cve_id]['max_criticality'].value:
                            cve_map[cve_id]['max_criticality'] = asset_crit
            
            for cve_id, data in cve_map.items():
                vuln = Vulnerability(
                    cve_id=cve_id,
                    cvss_score=data['cvss_score'],
                    cvss_version=data['cvss_version'],
                    affected_assets=list(set(data['assets'])),
                    asset_criticality=data['max_criticality'],
                    exposure_level=self.determine_exposure(data['assets']),
                    data_sensitivity=self.determine_sensitivity(data['assets']),
                    description=data['description'][:200],
                    plugin_id=data['plugin_id'],
                    port=data['port'],
                    protocol=data['protocol'],
                    source=source
                )
                vulnerabilities.append(vuln)
            
            print(f"[INFO] Imported {len(vulnerabilities)} unique CVEs from {source.value}")
            
        except Exception as e:
            print(f"[ERROR] Error importing {source.value} file: {e}")
            import traceback
            traceback.print_exc()
        
        return vulnerabilities
    
    def _parse_qualys_csv(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Parse Qualys CSV format"""
        vulnerabilities = []
        cve_map = {}
        
        try:
            import csv
            
            with open(filepath, 'r', encoding='utf-8') as f:
                # Skip the first line if it's a note
                first_line = f.readline()
                if first_line.startswith('"Note:'):
                    # Skip blank line too
                    f.readline()
                else:
                    # Reset if not a note
                    f.seek(0)
                
                reader = csv.DictReader(f)
                
                row_count = 0
                for row in reader:
                    row_count += 1
                    
                    # Extract key fields
                    cve_id = row.get('cveId', '').strip()
                    if not cve_id or cve_id == '':
                        continue
                    
                    asset_name = row.get('assetName', '').strip()
                    qds = row.get('qds', '0').strip()
                    title = row.get('title', '').strip()
                    criticality_score = row.get('asset.criticalityScore', '3').strip()
                    status = row.get('status', '').strip()
                    
                    # Skip inactive findings
                    if status != 'ACTIVE':
                        continue
                    
                    # Convert QDS to approximate CVSS score
                    # QDS ranges 0-100, we'll map to CVSS 0-10 scale
                    try:
                        qds_score = float(qds)
                        # Approximate mapping: QDS 0-100 -> CVSS 0-10
                        # QDS >= 90 -> Critical (9-10)
                        # QDS 70-89 -> High (7-8.9)
                        # QDS 40-69 -> Medium (4-6.9)
                        # QDS < 40 -> Low (0-3.9)
                        if qds_score >= 90:
                            cvss_score = 9.0 + (qds_score - 90) / 10.0
                        elif qds_score >= 70:
                            cvss_score = 7.0 + (qds_score - 70) / 10.0
                        elif qds_score >= 40:
                            cvss_score = 4.0 + (qds_score - 40) / 10.0
                        else:
                            cvss_score = qds_score / 10.0
                        cvss_score = min(10.0, max(0.0, cvss_score))
                    except ValueError:
                        cvss_score = 5.0  # Default medium severity
                    
                    # Determine asset criticality
                    try:
                        crit_value = int(criticality_score)
                        if crit_value >= 5:
                            asset_crit = AssetCriticality.CRITICAL
                        elif crit_value >= 4:
                            asset_crit = AssetCriticality.HIGH
                        elif crit_value >= 3:
                            asset_crit = AssetCriticality.MEDIUM
                        elif crit_value >= 2:
                            asset_crit = AssetCriticality.LOW
                        else:
                            asset_crit = AssetCriticality.MINIMAL
                    except (ValueError, TypeError):
                        asset_crit = AssetCriticality.MEDIUM
                    
                    # Track CVEs and affected assets
                    if cve_id not in cve_map:
                        cve_map[cve_id] = {
                            'cvss_score': cvss_score,
                            'qds_score': qds_score,
                            'cvss_version': '3.1',  # Qualys typically uses CVSS 3.x
                            'assets': [],
                            'description': title[:200],
                            'max_criticality': asset_crit
                        }
                    
                    cve_map[cve_id]['assets'].append(asset_name)
                    
                    # Track highest asset criticality
                    if asset_crit.value > cve_map[cve_id]['max_criticality'].value:
                        cve_map[cve_id]['max_criticality'] = asset_crit
                
                print(f"[INFO] Processed {row_count} rows from CSV")
            
            # Create vulnerability objects
            for cve_id, data in cve_map.items():
                vuln = Vulnerability(
                    cve_id=cve_id,
                    cvss_score=data['cvss_score'],
                    cvss_version=data['cvss_version'],
                    affected_assets=list(set(data['assets'])),
                    asset_criticality=data['max_criticality'],
                    exposure_level=self.determine_exposure(data['assets']),
                    data_sensitivity=self.determine_sensitivity(data['assets']),
                    description=data['description'],
                    source=source
                )
                vulnerabilities.append(vuln)
            
            print(f"[INFO] Imported {len(vulnerabilities)} unique CVEs from Qualys CSV")
            
        except Exception as e:
            print(f"[ERROR] Error importing Qualys CSV file: {e}")
            import traceback
            traceback.print_exc()
        
        return vulnerabilities
    
    
    def _parse_qualys_xml(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Parse Qualys XML format (placeholder - to be implemented)"""
        print(f"[WARNING] Qualys XML parsing not fully implemented yet")
        print(f"[INFO] Please export Qualys data as CSV format instead")
        return []
    
    def classify_asset(self, hostname: str) -> AssetCriticality:
        """Classify asset criticality based on hostname"""
        hostname_lower = hostname.lower()
        
        critical_keywords = ['prod', 'production', 'payment', 'customer', 'database', 'db', 'auth']
        high_keywords = ['web', 'api', 'app', 'server']
        low_keywords = ['dev', 'test', 'staging', 'demo']
        
        if any(kw in hostname_lower for kw in critical_keywords):
            return AssetCriticality.CRITICAL
        elif any(kw in hostname_lower for kw in low_keywords):
            return AssetCriticality.LOW
        elif any(kw in hostname_lower for kw in high_keywords):
            return AssetCriticality.HIGH
        else:
            return AssetCriticality.MEDIUM
    
    def determine_exposure(self, assets: List[str]) -> ExposureLevel:
        """Determine exposure level based on asset names"""
        assets_str = ' '.join(assets).lower()
        
        if any(kw in assets_str for kw in ['dmz', 'web', 'public', 'external', 'internet']):
            return ExposureLevel.INTERNET
        elif any(kw in assets_str for kw in ['vpn', 'partner', 'extranet']):
            return ExposureLevel.EXTRANET
        else:
            return ExposureLevel.INTERNAL
    
    def determine_sensitivity(self, assets: List[str]) -> DataSensitivity:
        """Determine data sensitivity based on asset names"""
        assets_str = ' '.join(assets).lower()
        
        if any(kw in assets_str for kw in ['payment', 'customer', 'database', 'auth', 'pii']):
            return DataSensitivity.HIGHLY_SENSITIVE
        elif any(kw in assets_str for kw in ['prod', 'production', 'internal']):
            return DataSensitivity.SENSITIVE
        else:
            return DataSensitivity.PUBLIC
    
    def calculate_cvss_component(self, vuln: Vulnerability) -> float:
        """Calculate CVSS component score (0-100)"""
        return (vuln.cvss_score / 10.0) * 100.0
    
    def calculate_epss_component(self, vuln: Vulnerability) -> float:
        """Calculate EPSS component score (0-100)"""
        if vuln.epss_score is None or vuln.epss_score == 0:
            return 0.0
        
        epss_base = vuln.epss_score * 100.0
        percentile_boost = (vuln.epss_percentile or 0) * 0.2
        
        return min(epss_base + percentile_boost, 100.0)
    
    def calculate_asset_context_component(self, vuln: Vulnerability) -> float:
        """Calculate asset context score (0-100)"""
        criticality_score = (vuln.asset_criticality.value / 5.0) * 40.0
        exposure_score = (vuln.exposure_level.value / 3.0) * 30.0
        sensitivity_score = (vuln.data_sensitivity.value / 3.0) * 30.0
        
        total = criticality_score + exposure_score + sensitivity_score
        
        if vuln.exposure_level == ExposureLevel.INTERNET:
            total *= self.config['asset_classification']['internet_facing_multiplier']
        
        if vuln.data_sensitivity == DataSensitivity.HIGHLY_SENSITIVE:
            total *= self.config['asset_classification']['data_sensitivity_multiplier']
        
        return min(total, 100.0)
    
    def calculate_threat_intel_component(self, vuln: Vulnerability) -> float:
        """Calculate threat intelligence component score (0-100)"""
        score = 0.0
        
        if vuln.in_cisa_kev:
            score += 40.0
        
        if vuln.actively_exploited:
            score += 30.0
        
        if vuln.exploit_available:
            score += 20.0
        
        if vuln.ransomware_campaign:
            score += 10.0
        
        return min(score, 100.0)
    
    def calculate_temporal_component(self, vuln: Vulnerability) -> float:
        """Calculate temporal component score (0-100)"""
        if not vuln.published_date:
            return 50.0
        
        days_since_publication = (datetime.now() - vuln.published_date).days
        
        if days_since_publication <= 7:
            return 100.0
        elif days_since_publication <= 30:
            return 80.0
        elif days_since_publication <= 90:
            return 60.0
        elif days_since_publication <= 180:
            return 40.0
        else:
            return 20.0
    
    def calculate_risk_score(self, vuln: Vulnerability) -> float:
        """Calculate overall risk score (0-100)"""
        weights = self.config['prioritization_weights']
        
        cvss_score = self.calculate_cvss_component(vuln)
        epss_score = self.calculate_epss_component(vuln)
        asset_score = self.calculate_asset_context_component(vuln)
        threat_score = self.calculate_threat_intel_component(vuln)
        temporal_score = self.calculate_temporal_component(vuln)
        
        risk_score = (
            cvss_score * weights['cvss_weight'] +
            epss_score * weights['epss_weight'] +
            asset_score * weights['asset_context_weight'] +
            threat_score * weights['threat_intel_weight'] +
            temporal_score * weights['temporal_weight']
        )
        
        return round(risk_score, 2)
    
    def determine_risk_level(self, risk_score: float) -> RiskLevel:
        """Determine risk level based on score"""
        if risk_score >= 80:
            return RiskLevel.CRITICAL
        elif risk_score >= 60:
            return RiskLevel.HIGH
        elif risk_score >= 40:
            return RiskLevel.MEDIUM
        elif risk_score >= 20:
            return RiskLevel.LOW
        else:
            return RiskLevel.MINIMAL
    
    def enrich_vulnerability(self, vuln: Vulnerability, epss_data: Dict, kev_set: set):
        """Enrich vulnerability with external data"""
        if vuln.cve_id in epss_data:
            vuln.epss_score = epss_data[vuln.cve_id]['epss']
            vuln.epss_percentile = epss_data[vuln.cve_id]['percentile']
        
        vuln.in_cisa_kev = vuln.cve_id in kev_set
        
        if vuln.in_cisa_kev:
            vuln.actively_exploited = True
    
    def prioritize_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Main prioritization function"""
        print(f"\n{'='*60}")
        print("VULNERABILITY PRIORITIZATION SYSTEM")
        print(f"{'='*60}\n")
        
        if not vulnerabilities:
            print("[INFO] No vulnerabilities to prioritize")
            return []
        
        # Collect unique CVEs
        cve_ids = list(set([v.cve_id for v in vulnerabilities]))
        print(f"[INFO] Processing {len(vulnerabilities)} vulnerability instances ({len(cve_ids)} unique CVEs)")
        
        # Fetch threat intelligence
        epss_data = self.fetch_epss_scores(cve_ids)
        kev_set = self.fetch_cisa_kev()
        
        print("\n[INFO] Calculating risk scores...")
        for vuln in vulnerabilities:
            self.enrich_vulnerability(vuln, epss_data, kev_set)
            vuln.risk_score = self.calculate_risk_score(vuln)
            vuln.risk_level = self.determine_risk_level(vuln.risk_score)
        
        # Sort by risk score
        vulnerabilities.sort(key=lambda v: v.risk_score, reverse=True)
        
        # Assign priority ranks
        for idx, vuln in enumerate(vulnerabilities, 1):
            vuln.priority_rank = idx
        
        print("[INFO] Prioritization complete")
        
        return vulnerabilities
    
    def generate_report(self, vulnerabilities: List[Vulnerability], top_n: int = 20) -> str:
        """Generate text report"""
        report = []
        report.append("\n" + "="*60)
        report.append("VULNERABILITY PRIORITIZATION REPORT")
        report.append("="*60)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total Vulnerabilities: {len(vulnerabilities)}")
        report.append("")
        
        # Risk distribution
        risk_counts = {}
        for level in RiskLevel:
            count = sum(1 for v in vulnerabilities if v.risk_level == level)
            risk_counts[level] = count
            percentage = (count / len(vulnerabilities) * 100) if vulnerabilities else 0
            report.append(f"{level.value}: {count} ({percentage:.1f}%)")
        
        report.append("\n" + "="*60)
        report.append(f"TOP {top_n} PRIORITY VULNERABILITIES")
        report.append("="*60)
        
        for vuln in vulnerabilities[:top_n]:
            report.append(f"\n#{vuln.priority_rank}. {vuln.cve_id} [Risk: {vuln.risk_score}/100] - {vuln.risk_level.value}")
            report.append(f"   CVSS: {vuln.cvss_score} | EPSS: {vuln.epss_score or 0:.4f} ({(vuln.epss_percentile or 0):.1f}%ile)")
            report.append(f"   Affected Assets: {len(vuln.affected_assets)} ({', '.join(vuln.affected_assets[:3])}...)")
            report.append(f"   Criticality: {vuln.asset_criticality.name} | Exposure: {vuln.exposure_level.name}")
            report.append(f"   Source: {vuln.source.value}")
            
            flags = []
            if vuln.in_cisa_kev:
                flags.append("CISA KEV")
            if vuln.actively_exploited:
                flags.append("ACTIVELY EXPLOITED")
            if vuln.exploit_available:
                flags.append("EXPLOIT AVAILABLE")
            
            if flags:
                report.append(f"   Flags: {' | '.join(flags)}")
            
            sla = self.get_sla_hours(vuln.risk_level)
            report.append(f"   Recommended Patch Window: {sla} hours ({sla//24} days)")
        
        return "\n".join(report)
    
    def get_sla_hours(self, risk_level: RiskLevel) -> int:
        """Get SLA hours based on risk level"""
        sla_map = {
            RiskLevel.CRITICAL: self.config['patch_management']['critical_sla_hours'],
            RiskLevel.HIGH: self.config['patch_management']['high_sla_hours'],
            RiskLevel.MEDIUM: self.config['patch_management']['medium_sla_hours'],
        }
        return sla_map.get(risk_level, 720)
    
    def export_to_csv(self, vulnerabilities: List[Vulnerability], filename: str = "prioritized_vulnerabilities.csv"):
        """Export to CSV file"""
        print(f"\n[INFO] Exporting to {filename}...")
        
        fieldnames = [
            'Priority_Rank', 'CVE_ID', 'Risk_Score', 'Risk_Level',
            'CVSS_Score', 'EPSS_Score', 'EPSS_Percentile',
            'Asset_Criticality', 'Exposure_Level', 'Data_Sensitivity',
            'In_CISA_KEV', 'Actively_Exploited', 'Exploit_Available',
            'Num_Affected_Assets', 'Sample_Assets', 'SLA_Hours', 'Source'
        ]
        
        with open(filename, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for vuln in vulnerabilities:
                writer.writerow({
                    'Priority_Rank': vuln.priority_rank,
                    'CVE_ID': vuln.cve_id,
                    'Risk_Score': vuln.risk_score,
                    'Risk_Level': vuln.risk_level.value,
                    'CVSS_Score': vuln.cvss_score,
                    'EPSS_Score': vuln.epss_score or 0,
                    'EPSS_Percentile': vuln.epss_percentile or 0,
                    'Asset_Criticality': vuln.asset_criticality.name,
                    'Exposure_Level': vuln.exposure_level.name,
                    'Data_Sensitivity': vuln.data_sensitivity.name,
                    'In_CISA_KEV': vuln.in_cisa_kev,
                    'Actively_Exploited': vuln.actively_exploited,
                    'Exploit_Available': vuln.exploit_available,
                    'Num_Affected_Assets': len(vuln.affected_assets),
                    'Sample_Assets': '; '.join(vuln.affected_assets[:5]),
                    'SLA_Hours': self.get_sla_hours(vuln.risk_level),
                    'Source': vuln.source.value
                })
        
        print(f"[SUCCESS] Exported {len(vulnerabilities)} vulnerabilities to {filename}")
    
    def export_to_json(self, vulnerabilities: List[Vulnerability], filename: str = "prioritized_vulnerabilities.json"):
        """Export to JSON file"""
        print(f"[INFO] Exporting to {filename}...")
        
        data = {
            'generated_at': datetime.now().isoformat(),
            'total_vulnerabilities': len(vulnerabilities),
            'vulnerabilities': []
        }
        
        for vuln in vulnerabilities:
            vuln_dict = {
                'priority_rank': vuln.priority_rank,
                'cve_id': vuln.cve_id,
                'risk_score': vuln.risk_score,
                'risk_level': vuln.risk_level.value,
                'cvss_score': vuln.cvss_score,
                'epss_score': vuln.epss_score,
                'epss_percentile': vuln.epss_percentile,
                'asset_criticality': vuln.asset_criticality.name,
                'exposure_level': vuln.exposure_level.name,
                'data_sensitivity': vuln.data_sensitivity.name,
                'in_cisa_kev': vuln.in_cisa_kev,
                'actively_exploited': vuln.actively_exploited,
                'exploit_available': vuln.exploit_available,
                'affected_assets': vuln.affected_assets[:10],
                'num_affected_assets': len(vuln.affected_assets),
                'sla_hours': self.get_sla_hours(vuln.risk_level),
                'source': vuln.source.value
            }
            data['vulnerabilities'].append(vuln_dict)
        
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"[SUCCESS] Exported to {filename}")
    
    def print_cache_stats(self):
        """Print cache statistics"""
        if self.db:
            stats = self.db.get_cache_stats()
            print("\n" + "="*60)
            print("CACHE STATISTICS")
            print("="*60)
            for key, value in stats.items():
                print(f"{key}: {value}")
            print("="*60)
    
    def __del__(self):
        """Cleanup database connection"""
        if self.db:
            self.db.close()


if __name__ == "__main__":
    print("[INFO] Use import_nessus(), import_tenable(), or import_qualys() methods to load scan files")
    print("[INFO] Specify data source with --source flag: nessus, tenable, qualys")
