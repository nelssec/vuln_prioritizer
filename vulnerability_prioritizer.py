#!/usr/bin/env python3

import json
import requests
import csv
from datetime import datetime, timedelta
from enum import Enum
from typing import List, Dict, Optional, Set
from dataclasses import dataclass
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import xml.etree.ElementTree as ET
from io import StringIO
import sqlite3
import os


class DataSource(Enum):
    NESSUS = "nessus"
    TENABLE = "tenable"
    QUALYS = "qualys"
    RAPID7 = "rapid7"
    GENERIC_CSV = "generic_csv"
    GENERIC_JSON = "generic_json"


class AssetCriticality(Enum):
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    MINIMAL = 1


class ExposureLevel(Enum):
    INTERNET = 3
    EXTRANET = 2
    INTERNAL = 1


class DataSensitivity(Enum):
    HIGHLY_SENSITIVE = 3
    SENSITIVE = 2
    PUBLIC = 1


class RiskLevel(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    MINIMAL = "MINIMAL"


@dataclass
class Vulnerability:
    cve_id: str
    cvss_score: float
    cvss_version: str = "3.1"
    affected_assets: List[str] = None
    asset_criticality: AssetCriticality = AssetCriticality.MEDIUM
    exposure_level: ExposureLevel = ExposureLevel.INTERNAL
    data_sensitivity: DataSensitivity = DataSensitivity.PUBLIC
    description: str = ""
    epss_score: Optional[float] = None
    epss_percentile: Optional[float] = None
    in_cisa_kev: bool = False
    exploit_available: bool = False
    actively_exploited: bool = False
    ransomware_campaign: bool = False
    published_date: Optional[datetime] = None
    last_modified_date: Optional[datetime] = None
    risk_score: float = 0.0
    risk_level: RiskLevel = RiskLevel.MEDIUM
    priority_rank: int = 0
    plugin_id: str = ""
    port: str = ""
    protocol: str = ""
    source: DataSource = DataSource.NESSUS
    qds_score: Optional[float] = None

    def __post_init__(self):
        if self.affected_assets is None:
            self.affected_assets = []


class EPSSCacheDB:
    
    def __init__(self, db_path="epss_cache.db"):
        self.db_path = db_path
        self.conn = None
        self.init_database()
    
    def init_database(self):
        try:
            self.conn = sqlite3.connect(self.db_path)
            cursor = self.conn.cursor()
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS epss_cache (
                    cve_id TEXT PRIMARY KEY,
                    score REAL,
                    percentile REAL,
                    model_version TEXT,
                    score_date TEXT,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS cisa_kev_cache (
                    cve_id TEXT PRIMARY KEY,
                    vendor_project TEXT,
                    product TEXT,
                    vulnerability_name TEXT,
                    date_added TEXT,
                    short_description TEXT,
                    required_action TEXT,
                    due_date TEXT,
                    cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS api_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    api_type TEXT,
                    url TEXT,
                    params TEXT,
                    status_code INTEGER,
                    response_time REAL,
                    from_cache BOOLEAN,
                    called_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            self.conn.commit()
            
        except Exception as e:
            self.conn = None
    
    def get_epss_score(self, cve_id, max_age_days=7):
        if not self.conn:
            return None
        
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT score, percentile, model_version, score_date 
                FROM epss_cache 
                WHERE cve_id = ? 
                AND datetime(cached_at) > datetime('now', ? || ' days')
            ''', (cve_id, -max_age_days))
            
            result = cursor.fetchone()
            if result:
                return {
                    'score': result[0],
                    'percentile': result[1],
                    'model_version': result[2],
                    'score_date': result[3]
                }
        except:
            pass
        
        return None
    
    def cache_epss_score(self, cve_id, score, percentile, model_version, score_date):
        if not self.conn:
            return
        
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO epss_cache 
                (cve_id, score, percentile, model_version, score_date)
                VALUES (?, ?, ?, ?, ?)
            ''', (cve_id, score, percentile, model_version, score_date))
            self.conn.commit()
        except:
            pass
    
    def get_all_cisa_kev_cves(self, max_age_days=1):
        if not self.conn:
            return None
        
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                SELECT cve_id FROM cisa_kev_cache
                WHERE datetime(cached_at) > datetime('now', ? || ' days')
            ''', (-max_age_days,))
            
            results = cursor.fetchall()
            if results:
                return set(row[0] for row in results)
        except:
            pass
        
        return None
    
    def cache_cisa_kev(self, cve_id, vendor_project, product, vulnerability_name, 
                       date_added, short_description, required_action, due_date):
        if not self.conn:
            return
        
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO cisa_kev_cache 
                (cve_id, vendor_project, product, vulnerability_name, 
                 date_added, short_description, required_action, due_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (cve_id, vendor_project, product, vulnerability_name,
                  date_added, short_description, required_action, due_date))
            self.conn.commit()
        except:
            pass
    
    def log_api_call(self, api_type, url, params, status_code, response_time, from_cache):
        if not self.conn:
            return
        
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT INTO api_logs 
                (api_type, url, params, status_code, response_time, from_cache)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (api_type, url, json.dumps(params), status_code, response_time, from_cache))
            self.conn.commit()
        except:
            pass
    
    def get_cache_stats(self):
        stats = {
            'epss_cache_count': 0,
            'kev_cache_count': 0,
            'api_call_count': 0,
            'last_epss_update': None,
            'last_kev_update': None
        }
        
        if not self.conn:
            return stats
        
        try:
            cursor = self.conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM epss_cache")
            stats['epss_cache_count'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM cisa_kev_cache")
            stats['kev_cache_count'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM api_logs")
            stats['api_call_count'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT MAX(cached_at) FROM epss_cache")
            result = cursor.fetchone()
            if result and result[0]:
                stats['last_epss_update'] = result[0]
            
            cursor.execute("SELECT MAX(cached_at) FROM cisa_kev_cache")
            result = cursor.fetchone()
            if result and result[0]:
                stats['last_kev_update'] = result[0]
            
        except:
            pass
        
        return stats
    
    def __del__(self):
        if self.conn:
            self.conn.close()


class VulnerabilityPrioritizer:
    def __init__(self, config_file: str = "prioritizer_config.json", 
                 use_cache: bool = True, cache_db_path: str = "epss_cache.db"):
                     
        self.config = self.load_config(config_file)
        self.use_cache = use_cache
        
        if use_cache:
            self.db = EPSSCacheDB(cache_db_path)
            print(f"[INFO] Using database cache at {cache_db_path}")
        else:
            self.db = None
            print("[INFO] Database caching disabled")
        
        self.epss_cache = {}
        self.cisa_kev_cache = set()
        
    def load_config(self, config_file: str) -> Dict:
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"[INFO] Config file {config_file} not found, using defaults")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        return {
            "prioritization_weights": {
                "cvss_weight": 0.15,
                "epss_weight": 0.30,
                "asset_context_weight": 0.25,
                "threat_intel_weight": 0.25,
                "temporal_weight": 0.05
            },
            "asset_classification": {
                "critical_indicators": ["prod", "payment", "customer", "database", "auth", "critical"],
                "internet_facing_multiplier": 1.5,
                "data_sensitivity_multiplier": 1.3,
                "critical_asset_multiplier": 1.4
            },
            "patch_management": {
                "critical_sla_hours": 24,
                "high_sla_hours": 168,
                "medium_sla_hours": 720
            },
            "api_settings": {
                "epss_api": "https://api.first.org/data/v1/epss",
                "cisa_kev_api": "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json",
                "timeout": 10,
                "max_retries": 3,
                "rate_limit_delay": 1.0
            },
            "cache_settings": {
                "epss_cache_days": 7,
                "kev_cache_days": 1
            },
            "critical_override_conditions": {
                "min_cvss_for_override": 7.0,
                "min_epss_for_override": 0.5,
                "require_cisa_kev": True,
                "critical_asset_auto_promote": True
            }
        }
    
    def fetch_epss_scores(self, cve_ids: List[str]) -> Dict[str, Dict]:
        print(f"[INFO] Fetching EPSS scores for {len(cve_ids)} CVEs...")
        epss_data = {}
        uncached_cves = []
        
        if self.db:
            cache_hits = 0
            for cve in cve_ids:
                cached = self.db.get_epss_score(
                    cve, 
                    max_age_days=self.config['cache_settings']['epss_cache_days']
                )
                if cached:
                    epss_data[cve] = {
                        'epss': cached['score'],
                        'percentile': cached['percentile'],
                        'model_version': cached['model_version'],
                        'score_date': cached['score_date']
                    }
                    cache_hits += 1
                else:
                    uncached_cves.append(cve)
            
            if cache_hits > 0:
                print(f"[CACHE] Found {cache_hits}/{len(cve_ids)} CVEs in cache")
        else:
            uncached_cves = cve_ids
        
        if not uncached_cves:
            return epss_data
        
        # API call for uncached CVEs
        print(f"[API] Fetching {len(uncached_cves)} CVEs from EPSS API...")
        success_count = 0
        
        # Process in batches
        batch_size = 50
        for i in range(0, len(uncached_cves), batch_size):
            batch = uncached_cves[i:i+batch_size]
            cve_param = ','.join(batch)
            
            try:
                headers = {
                    'User-Agent': 'VulnPrioritizer/1.0',
                    'Accept': 'application/json'
                }
                
                params = {'cve': cve_param}
                
                start_time = time.time()
                response = requests.get(
                    self.config['api_settings']['epss_api'],
                    params=params,
                    headers=headers,
                    timeout=self.config['api_settings']['timeout']
                )
                
                response_time = time.time() - start_time
                
                # Log API call
                if self.db:
                    self.db.log_api_call(
                        "EPSS",
                        self.config['api_settings']['epss_api'],
                        params,
                        response.status_code,
                        response_time,
                        False
                    )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if 'data' in data:
                        for item in data['data']:
                            cve = item.get('cve')
                            if cve:
                                epss_data[cve] = {
                                    'epss': float(item.get('epss', 0)),
                                    'percentile': float(item.get('percentile', 0)),
                                    'model_version': data.get('model_version', ''),
                                    'score_date': data.get('score_date', '')
                                }
                                success_count += 1
                                
                                # Cache the result
                                if self.db:
                                    self.db.cache_epss_score(
                                        cve,
                                        epss_data[cve]['epss'],
                                        epss_data[cve]['percentile'],
                                        epss_data[cve]['model_version'],
                                        epss_data[cve]['score_date']
                                    )
                else:
                    print(f"[ERROR] EPSS API returned status {response.status_code}")
                    
            except Exception as e:
                print(f"[ERROR] Error fetching EPSS batch: {e}")
            
            # Rate limiting
            time.sleep(self.config['api_settings']['rate_limit_delay'])
        
        print(f"[INFO] Successfully fetched EPSS for {success_count}/{len(uncached_cves)} CVEs from API")
        
        # Ensure all requested CVEs have data
        for cve in cve_ids:
            if cve not in epss_data:
                epss_data[cve] = {'epss': 0.0, 'percentile': 0.0, 
                                'model_version': '', 'score_date': ''}
        
        return epss_data
    
    def fetch_cisa_kev(self) -> set:
        """Fetch CISA Known Exploited Vulnerabilities catalog with database caching"""
        # Check memory cache first
        if self.cisa_kev_cache:
            return self.cisa_kev_cache
        
        # Check database cache
        if self.db:
            cached_kevs = self.db.get_all_cisa_kev_cves(
                max_age_days=self.config['cache_settings']['kev_cache_days']
            )
            if cached_kevs:
                print(f"[CACHE] Loaded {len(cached_kevs)} CVEs from CISA KEV cache")
                self.cisa_kev_cache = cached_kevs
                return self.cisa_kev_cache
        
        print("[API] Fetching CISA KEV catalog...")
        try:
            start_time = time.time()
            headers = {
                'User-Agent': 'VulnPrioritizer/1.0',
                'Accept': 'application/json'
            }
            
            response = requests.get(
                self.config['api_settings']['cisa_kev_api'],
                headers=headers,
                timeout=self.config['api_settings']['timeout']
            )
            
            response_time = time.time() - start_time
            
            # Log API call
            if self.db:
                self.db.log_api_call(
                    "CISA_KEV",
                    self.config['api_settings']['cisa_kev_api'],
                    {},
                    response.status_code,
                    response_time,
                    False
                )
            
            if response.status_code == 200:
                data = response.json()
                vulnerabilities = data.get('vulnerabilities', [])
                
                for vuln in vulnerabilities:
                    cve_id = vuln.get('cveID', '')
                    if cve_id:
                        self.cisa_kev_cache.add(cve_id)
                        
                        # Cache in database
                        if self.db:
                            self.db.cache_cisa_kev(
                                cve_id,
                                vuln.get('vendorProject', ''),
                                vuln.get('product', ''),
                                vuln.get('vulnerabilityName', ''),
                                vuln.get('dateAdded', ''),
                                vuln.get('shortDescription', ''),
                                vuln.get('requiredAction', ''),
                                vuln.get('dueDate', '')
                            )
                
                print(f"[INFO] Loaded {len(self.cisa_kev_cache)} CVEs from CISA KEV")
            else:
                print(f"[ERROR] Failed to fetch CISA KEV: {response.status_code}")
                
        except Exception as e:
            print(f"[ERROR] Error fetching CISA KEV: {e}")
            
        return self.cisa_kev_cache
    
    def import_nessus(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Nessus .nessus XML file"""
        print(f"\n[INFO] Importing Nessus scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.NESSUS)
    
    def import_tenable(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Tenable export file"""
        print(f"\n[INFO] Importing Tenable scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.TENABLE)
    
    def import_qualys(self, filepath: str) -> List[Vulnerability]:
        """Import vulnerabilities from Qualys export file"""
        print(f"\n[INFO] Importing Qualys scan: {filepath}")
        return self._import_vulnerability_scan(filepath, DataSource.QUALYS)
    
    def _import_vulnerability_scan(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Generic vulnerability scan importer"""
        if source == DataSource.NESSUS or source == DataSource.TENABLE:
            return self._parse_nessus_xml(filepath, source)
        elif source == DataSource.QUALYS:
            # Detect if it's CSV or XML based on file extension
            if filepath.lower().endswith('.csv'):
                return self._parse_qualys_csv(filepath, source)
            else:
                return self._parse_qualys_xml(filepath, source)
        else:
            print(f"[ERROR] Unsupported source: {source}")
            return []
    
    def _parse_nessus_xml(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Parse Nessus/Tenable XML format"""
        vulnerabilities = []
        cve_map = {}
        
        try:
            tree = ET.parse(filepath)
            root = tree.getroot()
            
            for host in root.findall('.//ReportHost'):
                hostname = host.get('name')
                asset_crit = self.classify_asset(hostname)
                
                for item in host.findall('.//ReportItem'):
                    severity = int(item.get('severity', 0))
                    if severity == 0:
                        continue
                    
                    cves = [cve.text for cve in item.findall('.//cve') if cve.text]
                    
                    if not cves:
                        continue
                    
                    for cve_id in cves:
                        cvss_elem = item.find('.//cvss3_base_score')
                        if cvss_elem is None:
                            cvss_elem = item.find('.//cvss_base_score')
                            cvss_version = "2.0"
                        else:
                            cvss_version = "3.1"
                        
                        cvss_score = float(cvss_elem.text) if cvss_elem is not None else 0.0
                        
                        if cvss_score == 0.0:
                            continue
                        
                        description = item.find('.//description')
                        desc_text = description.text if description is not None else ""
                        
                        plugin_id = item.get('pluginID', '')
                        port = item.get('port', '')
                        protocol = item.get('protocol', '')
                        
                        if cve_id not in cve_map:
                            cve_map[cve_id] = {
                                'cvss_score': cvss_score,
                                'cvss_version': cvss_version,
                                'assets': [],
                                'description': desc_text[:200],
                                'plugin_id': plugin_id,
                                'port': port,
                                'protocol': protocol,
                                'max_criticality': asset_crit
                            }
                        
                        cve_map[cve_id]['assets'].append(hostname)
                        
                        # Track highest asset criticality
                        if asset_crit.value > cve_map[cve_id]['max_criticality'].value:
                            cve_map[cve_id]['max_criticality'] = asset_crit
            
            # Create vulnerability objects
            for cve_id, data in cve_map.items():
                vuln = Vulnerability(
                    cve_id=cve_id,
                    cvss_score=data['cvss_score'],
                    cvss_version=data['cvss_version'],
                    affected_assets=list(set(data['assets'])),
                    asset_criticality=data['max_criticality'],
                    exposure_level=self.determine_exposure(data['assets']),
                    data_sensitivity=self.determine_sensitivity(data['assets']),
                    description=data['description'],
                    plugin_id=data['plugin_id'],
                    port=data['port'],
                    protocol=data['protocol'],
                    source=source
                )
                vulnerabilities.append(vuln)
            
            print(f"[INFO] Imported {len(vulnerabilities)} unique CVEs from {source.value} scan")
            
        except Exception as e:
            print(f"[ERROR] Error parsing {source.value} XML: {e}")
            import traceback
            traceback.print_exc()
        
        return vulnerabilities
    
    def _parse_qualys_csv(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        vulnerabilities = []
        cve_map = {}
        
        ACTIVE_STATUSES = {'ACTIVE', 'REOPENED', 'NEW', 'OPEN', 'RE-OPENED'}
        
        try:
            import csv
            
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                if first_line.startswith('"Note:'):
                    f.readline()
                else:
                    f.seek(0)
                
                reader = csv.DictReader(f)
                
                row_count = 0
                skipped_count = 0
                for row in reader:
                    row_count += 1
                    
                    # Extract key fields
                    cve_id = row.get('cveId', '').strip()
                    if not cve_id or cve_id == '':
                        skipped_count += 1
                        continue
                    
                    asset_name = row.get('assetName', '').strip()
                    qds = row.get('qds', '0').strip()
                    title = row.get('title', '').strip()
                    criticality_score = row.get('asset.criticalityScore', '3').strip()
                    status = row.get('status', '').strip().upper()
                    
                    # Include REOPENED and other active statuses
                    if status and status not in ACTIVE_STATUSES:
                        skipped_count += 1
                        continue
                    
                    # Convert QDS to approximate CVSS score with improved mapping
                    try:
                        qds_score = float(qds)
                        # Improved mapping for QDS to CVSS
                        if qds_score >= 95:
                            cvss_score = 9.5 + (qds_score - 95) / 10.0
                        elif qds_score >= 90:
                            cvss_score = 9.0 + (qds_score - 90) / 20.0
                        elif qds_score >= 70:
                            cvss_score = 7.0 + (qds_score - 70) / 10.0
                        elif qds_score >= 40:
                            cvss_score = 4.0 + (qds_score - 40) / 10.0
                        else:
                            cvss_score = qds_score / 10.0
                        cvss_score = min(10.0, max(0.0, cvss_score))
                    except ValueError:
                        cvss_score = 5.0  # Default medium severity
                        qds_score = 50.0
                    
                    try:
                        crit_value = int(criticality_score)
                        if crit_value == 5:
                            asset_crit = AssetCriticality.CRITICAL
                        elif crit_value == 4:
                            asset_crit = AssetCriticality.HIGH
                        elif crit_value == 3:
                            asset_crit = AssetCriticality.MEDIUM
                        elif crit_value == 2:
                            asset_crit = AssetCriticality.LOW
                        else:
                            asset_crit = AssetCriticality.MINIMAL
                    except (ValueError, TypeError):
                        asset_crit = AssetCriticality.MEDIUM
                    
                    # Track CVEs and affected assets
                    if cve_id not in cve_map:
                        cve_map[cve_id] = {
                            'cvss_score': cvss_score,
                            'qds_score': qds_score,
                            'cvss_version': '3.1',
                            'assets': [],
                            'description': title[:200],
                            'max_criticality': asset_crit,
                            'statuses': set()
                        }
                    
                    cve_map[cve_id]['assets'].append(asset_name)
                    cve_map[cve_id]['statuses'].add(status)
                    
                    # Track highest asset criticality
                    if asset_crit.value > cve_map[cve_id]['max_criticality'].value:
                        cve_map[cve_id]['max_criticality'] = asset_crit
                
                print(f"[INFO] Processed {row_count} rows from CSV ({skipped_count} skipped)")
                print(f"[INFO] Found {len(cve_map)} unique CVEs")
            
            # Create vulnerability objects
            for cve_id, data in cve_map.items():
                vuln = Vulnerability(
                    cve_id=cve_id,
                    cvss_score=data['cvss_score'],
                    cvss_version=data['cvss_version'],
                    affected_assets=list(set(data['assets'])),
                    asset_criticality=data['max_criticality'],
                    exposure_level=self.determine_exposure(data['assets']),
                    data_sensitivity=self.determine_sensitivity(data['assets']),
                    description=data['description'],
                    source=source,
                    qds_score=data.get('qds_score')
                )
                vulnerabilities.append(vuln)
                
            print(f"[INFO] Created {len(vulnerabilities)} vulnerability objects")
            
            # Print asset criticality distribution
            crit_dist = {}
            for vuln in vulnerabilities:
                crit_name = vuln.asset_criticality.name
                crit_dist[crit_name] = crit_dist.get(crit_name, 0) + 1
            
            print("[INFO] Asset Criticality Distribution:")
            for crit, count in sorted(crit_dist.items(), key=lambda x: x[0]):
                print(f"  - {crit}: {count} CVEs")
                
        except Exception as e:
            print(f"[ERROR] Error parsing Qualys CSV: {e}")
            import traceback
            traceback.print_exc()
            
        return vulnerabilities
    
    def _parse_qualys_xml(self, filepath: str, source: DataSource) -> List[Vulnerability]:
        """Parse Qualys XML format (placeholder - implement based on actual format)"""
        print(f"[WARN] Qualys XML parsing not fully implemented, using basic parser")
        return []
    
    def classify_asset(self, hostname: str) -> AssetCriticality:
        """Classify asset criticality based on hostname patterns"""
        hostname_lower = hostname.lower()
        
        critical_keywords = self.config['asset_classification']['critical_indicators']
        high_keywords = ['app', 'api', 'srv', 'server']
        low_keywords = ['dev', 'test', 'qa', 'stage', 'sandbox']
        
        if any(kw in hostname_lower for kw in critical_keywords):
            return AssetCriticality.CRITICAL
        elif any(kw in hostname_lower for kw in low_keywords):
            return AssetCriticality.LOW
        elif any(kw in hostname_lower for kw in high_keywords):
            return AssetCriticality.HIGH
        else:
            return AssetCriticality.MEDIUM
    
    def determine_exposure(self, assets: List[str]) -> ExposureLevel:
        assets_str = ' '.join(assets).lower()
        
        internet_keywords = ['dmz', 'web', 'public', 'external', 'internet', 'www', 
                           'api', 'portal', 'gateway', 'edge', 'perimeter']
        extranet_keywords = ['vpn', 'partner', 'extranet', 'vendor', 'b2b']
        
        if any(kw in assets_str for kw in internet_keywords):
            return ExposureLevel.INTERNET
        elif any(kw in assets_str for kw in extranet_keywords):
            return ExposureLevel.EXTRANET
        else:
            return ExposureLevel.INTERNAL
    
    def determine_sensitivity(self, assets: List[str]) -> DataSensitivity:
        assets_str = ' '.join(assets).lower()
        
        highly_sensitive_keywords = ['payment', 'customer', 'database', 'auth', 'pii', 
                                    'credit', 'ssn', 'medical', 'health', 'finance',
                                    'banking', 'payroll', 'hr']
        sensitive_keywords = ['prod', 'production', 'internal', 'corporate', 'business']
        
        if any(kw in assets_str for kw in highly_sensitive_keywords):
            return DataSensitivity.HIGHLY_SENSITIVE
        elif any(kw in assets_str for kw in sensitive_keywords):
            return DataSensitivity.SENSITIVE
        else:
            return DataSensitivity.PUBLIC
    
    def calculate_cvss_component(self, vuln: Vulnerability) -> float:
        if vuln.cvss_score >= 9.0:
            return 95.0 + (vuln.cvss_score - 9.0) * 5.0
        elif vuln.cvss_score >= 7.0:
            return 70.0 + (vuln.cvss_score - 7.0) * 12.5
        elif vuln.cvss_score >= 4.0:
            return 40.0 + (vuln.cvss_score - 4.0) * 10.0
        else:
            return vuln.cvss_score * 10.0
    
    def calculate_epss_component(self, vuln: Vulnerability) -> float:
        if vuln.epss_score is None or vuln.epss_score == 0:
            return 0.0
        
        if vuln.epss_score >= 0.5:
            base_score = 80.0 + (vuln.epss_score - 0.5) * 40.0
        elif vuln.epss_score >= 0.1:
            base_score = 50.0 + (vuln.epss_score - 0.1) * 75.0
        elif vuln.epss_score >= 0.01:
            base_score = 20.0 + (vuln.epss_score - 0.01) * 3333.0
        else:
            base_score = vuln.epss_score * 2000.0
        
        percentile_boost = (vuln.epss_percentile or 0) * 0.1
        return min(base_score + percentile_boost, 100.0)
    
    def calculate_asset_context_component(self, vuln: Vulnerability) -> float:
        criticality_scores = {
            AssetCriticality.CRITICAL: 100.0,
            AssetCriticality.HIGH: 75.0,
            AssetCriticality.MEDIUM: 50.0,
            AssetCriticality.LOW: 25.0,
            AssetCriticality.MINIMAL: 10.0
        }
        
        criticality_score = criticality_scores[vuln.asset_criticality]
        
        exposure_scores = {
            ExposureLevel.INTERNET: 100.0,
            ExposureLevel.EXTRANET: 60.0,
            ExposureLevel.INTERNAL: 30.0
        }
        exposure_score = exposure_scores[vuln.exposure_level]
        
        sensitivity_scores = {
            DataSensitivity.HIGHLY_SENSITIVE: 100.0,
            DataSensitivity.SENSITIVE: 60.0,
            DataSensitivity.PUBLIC: 20.0
        }
        sensitivity_score = sensitivity_scores[vuln.data_sensitivity]
        
        # Weighted combination (emphasizing criticality)
        base_score = (
            criticality_score * 0.5 +  # 50% weight on asset criticality
            exposure_score * 0.3 +     # 30% weight on exposure
            sensitivity_score * 0.2     # 20% weight on data sensitivity
        )
        
        # Apply multipliers for high-risk combinations
        if vuln.asset_criticality == AssetCriticality.CRITICAL:
            base_score *= self.config['asset_classification'].get('critical_asset_multiplier', 1.4)
        
        if vuln.exposure_level == ExposureLevel.INTERNET:
            base_score *= self.config['asset_classification']['internet_facing_multiplier']
        
        if vuln.data_sensitivity == DataSensitivity.HIGHLY_SENSITIVE:
            base_score *= self.config['asset_classification']['data_sensitivity_multiplier']
        
        return min(base_score, 100.0)
    
    def calculate_threat_intel_component(self, vuln: Vulnerability) -> float:
        score = 0.0
        
        if vuln.in_cisa_kev:
            score = 80.0
        
        if vuln.actively_exploited:
            score = max(score, 70.0)
            score += 10.0
        
        if vuln.exploit_available:
            score = max(score, 40.0)
            score += 10.0
        
        if vuln.ransomware_campaign:
            score += 20.0
        
        if hasattr(vuln, 'qds_score') and vuln.qds_score:
            if vuln.qds_score >= 90:
                score = max(score, 60.0)
        
        return min(score, 100.0)
    
    def calculate_temporal_component(self, vuln: Vulnerability) -> float:
        if not vuln.published_date:
            return 30.0
        
        days_since_publication = (datetime.now() - vuln.published_date).days
        
        if days_since_publication <= 7:
            return 100.0
        elif days_since_publication <= 30:
            return 80.0
        elif days_since_publication <= 90:
            return 60.0
        elif days_since_publication <= 365:
            return 40.0
        elif days_since_publication <= 730:
            return 30.0
        else:
            return 20.0
    
    def calculate_risk_score(self, vuln: Vulnerability) -> float:
        weights = self.config['prioritization_weights']
        override_config = self.config.get('critical_override_conditions', {})
        
        cvss_score = self.calculate_cvss_component(vuln)
        epss_score = self.calculate_epss_component(vuln)
        asset_score = self.calculate_asset_context_component(vuln)
        threat_score = self.calculate_threat_intel_component(vuln)
        temporal_score = self.calculate_temporal_component(vuln)
        
        risk_score = (
            cvss_score * weights['cvss_weight'] +
            epss_score * weights['epss_weight'] +
            asset_score * weights['asset_context_weight'] +
            threat_score * weights['threat_intel_weight'] +
            temporal_score * weights['temporal_weight']
        )
        
        # Override conditions
        if (vuln.cvss_score >= override_config.get('min_cvss_for_override', 7.0) and
            vuln.epss_score and vuln.epss_score >= override_config.get('min_epss_for_override', 0.5) and
            vuln.in_cisa_kev):
            risk_score = max(risk_score, 85.0)
        
        elif (vuln.asset_criticality == AssetCriticality.CRITICAL and
              vuln.cvss_score >= 7.0 and
              (vuln.in_cisa_kev or vuln.exploit_available or 
               (vuln.epss_score and vuln.epss_score >= 0.1))):
            risk_score = max(risk_score, 82.0)
        
        elif (vuln.exposure_level == ExposureLevel.INTERNET and
              vuln.cvss_score >= 8.0 and
              (vuln.exploit_available or vuln.in_cisa_kev)):
            risk_score = max(risk_score, 80.0)
        
        elif vuln.epss_score and vuln.epss_score >= 0.7:
            risk_score = max(risk_score, 65.0)
        
        elif vuln.asset_criticality == AssetCriticality.CRITICAL and vuln.cvss_score >= 9.0:
            risk_score = max(risk_score, 75.0)
        
        return round(risk_score, 2)
    
    def determine_risk_level(self, risk_score: float, vuln: Vulnerability = None) -> RiskLevel:
        if vuln and vuln.asset_criticality == AssetCriticality.CRITICAL:
            if risk_score >= 60:
                return RiskLevel.CRITICAL
            elif risk_score >= 40:
                return RiskLevel.HIGH
            elif risk_score >= 25:
                return RiskLevel.MEDIUM
            else:
                return RiskLevel.LOW
        
        if risk_score >= 80:
            return RiskLevel.CRITICAL
        elif risk_score >= 60:
            return RiskLevel.HIGH
        elif risk_score >= 40:
            return RiskLevel.MEDIUM
        elif risk_score >= 20:
            return RiskLevel.LOW
        else:
            return RiskLevel.MINIMAL
    
    def enrich_vulnerability(self, vuln: Vulnerability, epss_data: Dict, kev_set: set):
        """Enrich vulnerability with external data"""
        # Add EPSS data
        if vuln.cve_id in epss_data:
            vuln.epss_score = epss_data[vuln.cve_id]['epss']
            vuln.epss_percentile = epss_data[vuln.cve_id]['percentile']
        
        # Check CISA KEV
        vuln.in_cisa_kev = vuln.cve_id in kev_set
        
        # If in CISA KEV, mark as actively exploited
        if vuln.in_cisa_kev:
            vuln.actively_exploited = True
            vuln.exploit_available = True
    
    def prioritize_vulnerabilities(self, vulnerabilities: List[Vulnerability]) -> List[Vulnerability]:
        """Main prioritization function"""
        print(f"\n[PRIORITIZATION] Starting prioritization for {len(vulnerabilities)} vulnerabilities...")
        
        # Deduplicate by CVE ID
        unique_vulns = {}
        for vuln in vulnerabilities:
            if vuln.cve_id not in unique_vulns:
                unique_vulns[vuln.cve_id] = vuln
            else:
                # Merge assets and keep highest criticality
                existing = unique_vulns[vuln.cve_id]
                existing.affected_assets.extend(vuln.affected_assets)
                existing.affected_assets = list(set(existing.affected_assets))
                if vuln.asset_criticality.value > existing.asset_criticality.value:
                    existing.asset_criticality = vuln.asset_criticality
                    existing.exposure_level = vuln.exposure_level
                    existing.data_sensitivity = vuln.data_sensitivity
        
        vulnerabilities = list(unique_vulns.values())
        print(f"[INFO] {len(vulnerabilities)} unique CVEs after deduplication")
        
        # Fetch external data
        cve_ids = [v.cve_id for v in vulnerabilities]
        epss_data = self.fetch_epss_scores(cve_ids)
        kev_set = self.fetch_cisa_kev()
        
        # Enrich and score vulnerabilities
        print("[INFO] Calculating risk scores...")
        for vuln in vulnerabilities:
            self.enrich_vulnerability(vuln, epss_data, kev_set)
            vuln.risk_score = self.calculate_risk_score(vuln)
            vuln.risk_level = self.determine_risk_level(vuln.risk_score, vuln)
        
        # Sort by risk score (highest first)
        vulnerabilities.sort(key=lambda x: x.risk_score, reverse=True)
        
        # Assign priority ranks
        for i, vuln in enumerate(vulnerabilities, 1):
            vuln.priority_rank = i
        
        # Print summary statistics
        risk_levels = {}
        for vuln in vulnerabilities:
            level = vuln.risk_level.value
            risk_levels[level] = risk_levels.get(level, 0) + 1
        
        print("\n[SUMMARY] Risk Level Distribution:")
        for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']:
            count = risk_levels.get(level, 0)
            percentage = (count / len(vulnerabilities) * 100) if vulnerabilities else 0
            print(f"  {level:8s}: {count:4d} ({percentage:5.1f}%)")
        
        return vulnerabilities
    
    def generate_report(self, vulnerabilities: List[Vulnerability], top_n: int = 20) -> str:
        report = []
        report.append("="*80)
        report.append(f" TOP {top_n} PRIORITY VULNERABILITIES")
        report.append("="*80)
        report.append("")
        
        for vuln in vulnerabilities[:top_n]:
            report.append(f"Rank #{vuln.priority_rank}: {vuln.cve_id}")
            report.append(f"  Risk Score: {vuln.risk_score:.1f}/100 ({vuln.risk_level.value})")
            report.append(f"  CVSS: {vuln.cvss_score:.1f}")
            
            if vuln.epss_score:
                report.append(f"  EPSS: {vuln.epss_score:.1%} (Percentile: {vuln.epss_percentile:.0f})")
            
            if vuln.qds_score:
                report.append(f"  QDS: {vuln.qds_score:.0f}")
            
            report.append(f"  Asset Criticality: {vuln.asset_criticality.name}")
            report.append(f"  Exposure: {vuln.exposure_level.name}")
            
            if vuln.in_cisa_kev:
                report.append("  [!] IN CISA KEV - KNOWN EXPLOITED")
            
            if vuln.actively_exploited:
                report.append("  [!] ACTIVELY EXPLOITED IN THE WILD")
            
            report.append(f"  Affected Assets: {len(vuln.affected_assets)}")
            
            if vuln.description:
                report.append(f"  Description: {vuln.description[:100]}...")
            
            report.append("")
        
        return "\n".join(report)
    
    def export_to_csv(self, vulnerabilities: List[Vulnerability], output_file: str):
        """Export prioritized vulnerabilities to CSV"""
        try:
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = [
                    'priority_rank', 'cve_id', 'risk_score', 'risk_level',
                    'cvss_score', 'epss_score', 'epss_percentile', 'qds_score',
                    'in_cisa_kev', 'actively_exploited', 'exploit_available',
                    'asset_criticality', 'exposure_level', 'data_sensitivity',
                    'affected_assets_count', 'affected_assets', 'description'
                ]
                
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for vuln in vulnerabilities:
                    writer.writerow({
                        'priority_rank': vuln.priority_rank,
                        'cve_id': vuln.cve_id,
                        'risk_score': vuln.risk_score,
                        'risk_level': vuln.risk_level.value,
                        'cvss_score': vuln.cvss_score,
                        'epss_score': vuln.epss_score or 0,
                        'epss_percentile': vuln.epss_percentile or 0,
                        'qds_score': getattr(vuln, 'qds_score', 0) or 0,
                        'in_cisa_kev': vuln.in_cisa_kev,
                        'actively_exploited': vuln.actively_exploited,
                        'exploit_available': vuln.exploit_available,
                        'asset_criticality': vuln.asset_criticality.name,
                        'exposure_level': vuln.exposure_level.name,
                        'data_sensitivity': vuln.data_sensitivity.name,
                        'affected_assets_count': len(vuln.affected_assets),
                        'affected_assets': '|'.join(vuln.affected_assets[:10]),
                        'description': vuln.description[:200]
                    })
            
            print(f"[EXPORT] Saved CSV to {output_file}")
            
        except Exception as e:
            print(f"[ERROR] Failed to export CSV: {e}")
    
    def export_to_json(self, vulnerabilities: List[Vulnerability], output_file: str):
        """Export prioritized vulnerabilities to JSON"""
        try:
            data = []
            for vuln in vulnerabilities:
                data.append({
                    'priority_rank': vuln.priority_rank,
                    'cve_id': vuln.cve_id,
                    'risk_score': vuln.risk_score,
                    'risk_level': vuln.risk_level.value,
                    'cvss_score': vuln.cvss_score,
                    'epss_score': vuln.epss_score,
                    'epss_percentile': vuln.epss_percentile,
                    'qds_score': getattr(vuln, 'qds_score', None),
                    'in_cisa_kev': vuln.in_cisa_kev,
                    'actively_exploited': vuln.actively_exploited,
                    'exploit_available': vuln.exploit_available,
                    'asset_criticality': vuln.asset_criticality.name,
                    'exposure_level': vuln.exposure_level.name,
                    'data_sensitivity': vuln.data_sensitivity.name,
                    'affected_assets': vuln.affected_assets,
                    'description': vuln.description
                })
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
            
            print(f"[EXPORT] Saved JSON to {output_file}")
            
        except Exception as e:
            print(f"[ERROR] Failed to export JSON: {e}")
    
    def print_cache_stats(self):
        """Print cache statistics"""
        if self.db:
            stats = self.db.get_cache_stats()
            print("\n" + "="*70)
            print(" DATABASE CACHE STATISTICS")
            print("="*70)
            print(f"EPSS Cache Entries: {stats['epss_cache_count']}")
            print(f"CISA KEV Entries: {stats['kev_cache_count']}")
            print(f"API Call Logs: {stats['api_call_count']}")
            if stats['last_epss_update']:
                print(f"Last EPSS Update: {stats['last_epss_update']}")
            if stats['last_kev_update']:
                print(f"Last KEV Update: {stats['last_kev_update']}")
